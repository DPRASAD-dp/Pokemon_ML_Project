{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjMiRI5ynRh2nPXtl3dkga"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["ADASYN and Near Miss Resampling"],"metadata":{"id":"Ret91SA-0JSI"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from imblearn.over_sampling import ADASYN\n","from imblearn.under_sampling import NearMiss\n","from imblearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder\n","\n","def sample_data(data, minority_class=1):\n","    \"\"\"\n","    Samples the data in a Pandas DataFrame using both ADASYN and NearMiss.\n","\n","    Args:\n","        data: A Pandas DataFrame containing the data (assumed to be loaded from a file).\n","        minority_class: The value representing the minority class in the target labels. (default: 1)\n","\n","    Returns:\n","        A Pandas DataFrame containing the sampled data.\n","    \"\"\"\n","    # Identify the target variable column name\n","    target_column_name = 'is_legendary'\n","    X = data.drop(target_column_name, axis=1)  # Drop the target variable column\n","    y = data[target_column_name]  # Get the target variable column\n","\n","    # Identify categorical/string columns\n","    categorical_cols = X.dtypes == object\n","    categorical_col_names = X.columns[categorical_cols].tolist()\n","\n","    # Reset the index of X before indexing with categorical_col_names\n","    X = X.reset_index(drop=True)\n","\n","    # Convert categorical columns to one-hot encoded numerical columns\n","    encoder = OneHotEncoder(handle_unknown='ignore')\n","    X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_col_names]).toarray(), columns=encoder.get_feature_names_out(categorical_col_names))\n","\n","    # Combine one-hot encoded columns with numerical columns\n","    X = pd.concat([X_encoded, X.drop(categorical_col_names, axis=1)], axis=1)\n","\n","    # Create a pipeline with both ADASYN and NearMiss\n","    sampling_pipeline = Pipeline([\n","        ('adasyn', ADASYN(sampling_strategy='auto', random_state=42)),\n","        ('nearmiss', NearMiss(version=3, sampling_strategy='auto'))\n","    ])\n","\n","    X_res, y_res = sampling_pipeline.fit_resample(X, y)\n","    sampled_data = pd.concat([X_res, pd.DataFrame(y_res, columns=[target_column_name])], axis=1)\n","    return sampled_data\n","\n","# Example usage\n","try:\n","    # Assuming the data is loaded from a file (replace 'creditcard.csv' with the actual filename)\n","    data = pd.read_csv('pokemon_modified.csv')\n","    print(data.columns)  # Print the column names to identify the target variable column\n","\n","    sampled_data = sample_data(data.copy())\n","\n","    # Export the sampled data to a new CSV file (replace 'sampled_data.csv' with the desired filename)\n","    sampled_data.to_csv('pokemon_resampled.csv', index=False)\n","    print(\"Sampled data saved to 'pokemon_resampled_data.csv'.\")\n","except FileNotFoundError:\n","    print(\"Error: Could not find the data file.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNw-_YN20O8Q","executionInfo":{"status":"ok","timestamp":1718621414968,"user_tz":-330,"elapsed":4171,"user":{"displayName":"DURGA PRASAD KAVALI","userId":"12339152402857825267"}},"outputId":"5319d844-6aee-4c74-f144-c9a55dcb730a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['against_bug', 'against_dark', 'against_dragon', 'against_electric',\n","       'against_fairy', 'against_fight', 'against_fire', 'against_flying',\n","       'against_ghost', 'against_grass', 'against_ground', 'against_ice',\n","       'against_normal', 'against_poison', 'against_psychic', 'against_rock',\n","       'against_steel', 'against_water', 'attack', 'base_egg_steps',\n","       'base_happiness', 'base_total', 'capture_rate', 'defense',\n","       'experience_growth', 'height_m', 'hp', 'percentage_male',\n","       'pokedex_number', 'sp_attack', 'sp_defense', 'speed', 'weight_kg',\n","       'generation', 'is_legendary', 'type1_en', 'type2_en'],\n","      dtype='object')\n","Sampled data saved to 'pokemon_resampled_data.csv'.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["pip install scikeras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":775},"id":"v_OlFUoW1FVh","executionInfo":{"status":"ok","timestamp":1718621643458,"user_tz":-330,"elapsed":15352,"user":{"displayName":"DURGA PRASAD KAVALI","userId":"12339152402857825267"}},"outputId":"d49b1d04-20df-4334-9202-ba78d1e4df4f","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikeras\n","  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n","Collecting keras>=3.2.0 (from scikeras)\n","  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn>=1.4.2 (from scikeras)\n","  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.25.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n","Collecting namex (from keras>=3.2.0->scikeras)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.9.0)\n","Collecting optree (from keras>=3.2.0->scikeras)\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.2.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.11.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n","Installing collected packages: namex, optree, scikit-learn, keras, scikeras\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0 scikeras-0.13.0 scikit-learn-1.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]},"id":"e7372cf377c24dc5a17f3196e73e18f9"}},"metadata":{}}]},{"cell_type":"markdown","source":["Ensemble model of Logistic Regression and LSTM"],"metadata":{"id":"QCZYx5m_0gTL"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout, Bidirectional, Concatenate, Input, Attention\n","from keras.optimizers import Adam\n","from scikeras.wrappers import KerasClassifier\n","from keras.callbacks import LearningRateScheduler\n","\n","# Define a learning rate schedule function\n","def lr_schedule(epoch):\n","    \"\"\"\n","    Learning rate schedule function.\n","    Adjust the learning rate based on the epoch.\n","    \"\"\"\n","    lr = 0.001  # Initial learning rate\n","    if epoch > 10:\n","        lr *= 0.5  # Reduce learning rate by half after 10 epochs\n","    return lr\n","\n","# Create LearningRateScheduler callback\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","# Read the uploaded file into a pandas dataframe\n","df = pd.read_csv(\"pokemon_resampled.csv\")\n","\n","# Handle missing values if any\n","imputer = SimpleImputer(strategy='most_frequent')\n","df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n","\n","# Perform one-hot encoding for categorical variables\n","df = pd.get_dummies(df)\n","print(df)\n","print(df.columns)\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(df.drop('is_legendary', axis=1), df['is_legendary'], test_size=0.33, random_state=17)\n","\n","# Preprocess structured data using Logistic Regression\n","numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n","categorical_features = X_train.select_dtypes(include=['object']).columns\n","\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),\n","    ('scaler', StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('encoder', OneHotEncoder())\n","])\n","\n","preprocessor_lr = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Define Logistic Regression model\n","lr_model = Pipeline(steps=[('preprocessor', preprocessor_lr), ('clf', LogisticRegression())])\n","\n","# Train Logistic Regression model\n","lr_model.fit(X_train, y_train)\n","\n","# Predict using Logistic Regression model\n","y_pred_lr = lr_model.predict(X_test[:y_test.shape[0]])\n","\n","# Preprocess sequential data using LSTM\n","preprocessor_lstm = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ])\n","\n","# Define the LSTM model input shape\n","input_shape = (X_train.shape[1], 1)  # (number_of_features, 1)\n","\n","# Define the LSTM model using the correct input shape\n","lstm_model = Sequential([\n","    LSTM(units=64, input_shape=input_shape),\n","    Dropout(0.2),\n","    Dense(units=1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Now, fit the model with the callbacks and other parameters\n","lstm_model.fit(X_train, y_train, epochs=30, batch_size=64, callbacks=[lr_scheduler], verbose=0)\n","\n","# Predict using LSTM model\n","y_pred_lstm = (lstm_model.predict(X_test[:y_test.shape[0]]) > 0.5).astype(\"int32\").reshape(-1)\n","\n","# Print shapes for debugging\n","print(\"Shape of y_test:\", y_test.shape)\n","print(\"Shape of y_pred_lr:\", y_pred_lr.shape)\n","print(\"Shape of y_pred_lstm:\", y_pred_lstm.shape)\n","\n","# Concatenate predictions from LR and LSTM models\n","concatenated_predictions = np.column_stack((y_pred_lr, y_pred_lstm))\n","\n","# Attention mechanism\n","attention_layer = Dense(1, activation='tanh')(concatenated_predictions)\n","attention_weight = Dense(1, activation='softmax')(attention_layer)\n","weighted_predictions = np.column_stack((y_pred_lr * attention_weight[:, 0], y_pred_lstm * (1 - attention_weight[:, 0])))\n","ensemble_pred = np.sum(weighted_predictions, axis=1)\n","\n","# Convert predictions to binary\n","ensemble_pred_binary = (ensemble_pred > 0.5).astype(\"int32\")\n","\n","# Print shapes for debugging\n","print(\"Shape of ensemble_pred:\", ensemble_pred.shape)\n","print(\"Shape of ensemble_pred_binary:\", ensemble_pred_binary.shape)\n","\n","# Calculate accuracy and additional evaluation metrics of the ensemble model\n","ensemble_accuracy = accuracy_score(y_test, ensemble_pred_binary)\n","roc_auc = roc_auc_score(y_test, ensemble_pred)\n","precision, recall, f1_score, _ = classification_report(y_test, ensemble_pred_binary, output_dict=True)['1'].values()\n","\n","print(\"Confusion matrix:\\n\", confusion_matrix(y_test, ensemble_pred_binary))\n","\n","# Store the result of the confusion matrix in a variable with a different name\n","conf_matrix = confusion_matrix(y_test, ensemble_pred_binary)\n","print(\"Confusion matrix:\\n\", conf_matrix)\n","\n","# Access elements of the confusion matrix using the new variable name\n","TN = conf_matrix[0][0]\n","FP = conf_matrix[0][1]\n","FN = conf_matrix[1][0]\n","TP = conf_matrix[1][1]\n","\n","\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(f\"Sensitivity: {sensitivity:.4f}\")\n","print(f\"Specificity: {specificity:.4f}\")\n","\n","# Print evaluation metrics\n","print(\"Ensemble Accuracy:\", ensemble_accuracy)\n","print(\"ROC AUC:\", roc_auc)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1_score)\n","print(\"Classification report:\\n\", classification_report(y_test, ensemble_pred_binary))\n","print(\"AUC:\", roc_auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwKI6RJD0bZo","executionInfo":{"status":"ok","timestamp":1718622905202,"user_tz":-330,"elapsed":10489,"user":{"displayName":"DURGA PRASAD KAVALI","userId":"12339152402857825267"}},"outputId":"779ddef3-b90a-42d4-a73d-953a0198b6e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     against_bug  against_dark  against_dragon  against_electric  \\\n","0       1.000000      1.000000        1.000000          1.000000   \n","1       1.000000      1.000000        1.000000          1.000000   \n","2       1.000000      1.000000        1.000000          1.000000   \n","3       2.000000      0.500000        1.000000          1.000000   \n","4       0.500000      1.000000        1.000000          2.000000   \n","..           ...           ...             ...               ...   \n","856     0.500000      0.500000        0.000000          1.873853   \n","857     0.989650      0.989650        0.326434          1.673566   \n","858     1.339766      1.339766        0.559844          1.440156   \n","859     0.557922      0.557922        0.000000          1.884156   \n","860     0.688348      0.688348        0.000000          1.623305   \n","\n","     against_fairy  against_fight  against_fire  against_flying  \\\n","0              1.0       2.000000      1.000000        1.000000   \n","1              1.0       2.000000      2.000000        1.000000   \n","2              1.0       2.000000      1.000000        1.000000   \n","3              2.0       4.000000      2.000000        1.000000   \n","4              1.0       0.250000      2.000000        2.000000   \n","..             ...            ...           ...             ...   \n","856            1.0       0.500000      0.542049        0.957951   \n","857            1.0       0.500000      0.663217        1.000000   \n","858            1.0       0.500000      0.779922        1.000000   \n","859            1.0       0.471039      0.557922        1.000000   \n","860            1.0       0.405826      0.688348        1.000000   \n","\n","     against_ghost  against_grass  ...  percentage_male  pokedex_number  \\\n","0         0.000000       1.000000  ...              0.0             440   \n","1         1.000000       1.000000  ...             88.1             471   \n","2         0.000000       1.000000  ...             50.0             428   \n","3         0.500000       1.000000  ...             50.0             461   \n","4         1.000000       0.250000  ...              0.0             416   \n","..             ...            ...  ...              ...             ...   \n","856       1.000000       1.915902  ...             50.0             787   \n","857       1.326434       1.673566  ...             50.0             788   \n","858       1.559844       1.440156  ...             50.0             788   \n","859       1.115844       1.884156  ...             50.0             787   \n","860       1.376695       1.623305  ...             50.0             787   \n","\n","     sp_attack  sp_defense     speed  weight_kg  generation  type1_en  \\\n","0    -1.086957   -0.025000 -0.875000  -0.021073           4        12   \n","1     1.413043    0.725000  0.000000   0.007663           4        11   \n","2    -0.239130    0.750000  1.750000   0.149425           4        12   \n","3    -0.434783    0.475000  1.500000   0.162835           4         1   \n","4     0.326087    0.900000 -0.625000   0.249042           4         0   \n","..         ...         ...       ...        ...         ...       ...   \n","856   0.652174    1.484365  0.594611  -0.083503           7        15   \n","857   0.183813    0.792077  0.108280  -0.214325           7        16   \n","858  -0.151081    0.214385 -0.171813  -0.308673           7        15   \n","859   0.740316    1.556559  0.528961  -0.088145           7        16   \n","860   0.938790    1.458739  0.594174  -0.101138           7        15   \n","\n","     type2_en  is_legendary  \n","0           7             0  \n","1           7             0  \n","2           7             0  \n","3          11             0  \n","4           7             0  \n","..        ...           ...  \n","856         4             1  \n","857         4             1  \n","858         5             1  \n","859         4             1  \n","860         4             1  \n","\n","[861 rows x 37 columns]\n","Index(['against_bug', 'against_dark', 'against_dragon', 'against_electric',\n","       'against_fairy', 'against_fight', 'against_fire', 'against_flying',\n","       'against_ghost', 'against_grass', 'against_ground', 'against_ice',\n","       'against_normal', 'against_poison', 'against_psychic', 'against_rock',\n","       'against_steel', 'against_water', 'attack', 'base_egg_steps',\n","       'base_happiness', 'base_total', 'capture_rate', 'defense',\n","       'experience_growth', 'height_m', 'hp', 'percentage_male',\n","       'pokedex_number', 'sp_attack', 'sp_defense', 'speed', 'weight_kg',\n","       'generation', 'type1_en', 'type2_en', 'is_legendary'],\n","      dtype='object')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","Shape of y_test: (285,)\n","Shape of y_pred_lr: (285,)\n","Shape of y_pred_lstm: (285,)\n","Shape of ensemble_pred: (285,)\n","Shape of ensemble_pred_binary: (285,)\n","Confusion matrix:\n"," [[ 34   2]\n"," [  3 246]]\n","Confusion matrix:\n"," [[ 34   2]\n"," [  3 246]]\n","Sensitivity: 0.9880\n","Specificity: 0.9444\n","Ensemble Accuracy: 0.9824561403508771\n","ROC AUC: 0.9991075412762159\n","Precision: 0.9919354838709677\n","Recall: 0.9879518072289156\n","F1 Score: 0.9899396378269618\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","           0       0.92      0.94      0.93        36\n","           1       0.99      0.99      0.99       249\n","\n","    accuracy                           0.98       285\n","   macro avg       0.96      0.97      0.96       285\n","weighted avg       0.98      0.98      0.98       285\n","\n","AUC: 0.9991075412762159\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (285, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n","  warnings.warn(\n"]}]}]}